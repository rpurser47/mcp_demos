import streamlit as st
from langgraph.graph import StateGraph, END
from langgraph_nodes import ollama_llm_node, llm_router_node, weather_tool_node
from typing import TypedDict, Any

# --- State schema ---
class ChatState(TypedDict, total=False):
    input: str
    system_prompt: str
    output: str
    tool_args: dict
    weather_result: Any

# --- Load system prompt ---
def load_system_prompt():
    with open("chatbot_system_prompt.md", "r", encoding="utf-8") as f:
        return f.read()

system_prompt = load_system_prompt()

# --- Streamlit page config ---
st.set_page_config(
    page_title="Weather Chatbot (Llama 3.2 + MCP)",
    page_icon="☁️",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None
)

st.title("☁️ Weather Chatbot (Llama 3.2 + MCP)")
st.caption("Ask about the weather anywhere in the US. Powered by local Llama 3 via Ollama, and your MCP server.")

# --- Ensure chat history is initialized ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

# --- LangGraph workflow ---
graph = StateGraph(state_schema=ChatState)
graph.add_node("llm", ollama_llm_node)
graph.add_node("router", llm_router_node)
graph.add_node("get_weather", weather_tool_node)

graph.add_edge("llm", "router")
graph.add_conditional_edges("router", lambda state: state["next"])
graph.add_edge("get_weather", "llm")
graph.set_entry_point("llm")
chat_graph = graph.compile()

# --- User input ---
if prompt := st.chat_input("Ask me about the weather..."):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    try:
        with st.spinner("Llama 3.2 is thinking..."):
            state = {"input": prompt, "system_prompt": system_prompt}
            print("[DEBUG] Invoking chat_graph with state:", state["input"])
            try:
                result = chat_graph.invoke(state)
                print("[DEBUG] chat_graph result:", result["output"])
            except Exception as e:
                st.error(f"[ERROR] Workflow execution failed: {e}")
                st.session_state["messages"].append({"role": "assistant", "content": "Sorry, something went wrong while processing your request. Please try again."})
                st.chat_message("assistant").write("Sorry, something went wrong while processing your request. Please try again.")
                raise
            response = result.get("output", "")
            if not response:
                st.warning("No response generated by the assistant.")
                response = "Sorry, I couldn't generate a response. Please try again."
    except Exception as e:
        st.error(f"[ERROR] Unexpected error: {e}")
        response = "Sorry, an unexpected error occurred. Please try again."
    st.session_state["messages"].append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)


# --- Dark mode note ---
st.markdown("<style>body { background-color: #18191A !important; color: #fff !important; }</style>", unsafe_allow_html=True)
